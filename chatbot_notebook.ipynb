{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[6950,   19,  544,  366,  304,  929,   38]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jtlor\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Get both the pretrained model and the pretrained tokenizer for this model anme\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "# Every time you interact with the chat bot, you will pass it all of the conversation history\n",
    "# where each element will be separated by \"\\n\"\n",
    "history_string = \"\\n\".join(conversation_history)\n",
    "\n",
    "# Example of input\n",
    "input_text = \"Hello, how are you doing?\"\n",
    "\n",
    "# Normally, tokenizer have a methdo called \"encode_plus\" that is used to tokenize or vectorize\n",
    "# the input to the model.\n",
    "#\n",
    "# Here, I am passing it the history and the current input and telling it to return it as \n",
    "# a PyTorch tensor using \"return_tensor = \"pt\"\"\n",
    "inputs  = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing so, you've now created a Python dictionary which contains special keywords that allow the model to properly reference its contents.\n",
    "\n",
    "To learn more about tokens and their associated pretrained vocabulary files, you can explore the pretrained_vocab_files_map attribute. This attribute provides a mapping of pretrained models to their corresponding vocabulary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pretrained_vocab_files_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you formatted the inputs, you can pass them to the model\n",
    "To pass the inputs to the model, check the documentation to see what method you should use and what the input should look like. In this case it is generate. \n",
    "\n",
    "Remember that **inputs breaks down the dictionary by giving each key word a variable in the method which's value will be the value that they key holds in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1, 281, 476, 929, 731,  21, 281, 632, 929, 712, 731,  21, 855, 366,\n",
      "         304,  38, 281, 632, 584,  21,   2]])\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, decode the output.\n",
    "(.strip() only makes the text look preattier by removing unnessesary spaces...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well. I am doing very well. How are you? I am good.\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, how are you doing?', \"I'm doing well. I am doing very well. How are you? I am good.\"]\n"
     ]
    }
   ],
   "source": [
    "conversation_history.append(input_text)\n",
    "conversation_history.append(response)\n",
    "print(conversation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, you were able to give the chat bot one input, but you want to be able to have smooth a back and forth conversation with it. One way to do this, is by putting the code in a while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    history_string = \"\\n\".join(conversation_history)\n",
    "\n",
    "    input_text = input(\"> \")\n",
    "\n",
    "    inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"tp\")\n",
    "\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    conversation_history.append(input_text)\n",
    "    conversation_history.append(response)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
